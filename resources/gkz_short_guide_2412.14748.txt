A SHORT GUIDE TO GKZ

arXiv:2412.14748v1 [math.AG] 19 Dec 2024

ED SEGAL

Abstract. These notes are a brief summary of the main results from the book
‘Discriminants, Resultants and Multidimensional Determinants’ by Gelfand–
Kapranov–Zelevinsky. We sketch the key ideas involved in the proofs, using
as little technical background as possible.

Contents
1.

Introduction
1.1.
1.2.

2.

Proof of the main theorem
2.1.
2.2.
2.3.
2.4.

3.

The discriminant of a cubic
More general discriminants
Projective duality
Toric degenerations
Associated hypersurfaces
From Chow forms to discriminants

Other topics
3.1.
3.2.

The Cayley method
Mirror symmetry

References

1
1
3
6
6
8
11
14
18
18
20
21

1. Introduction
The purpose of these notes is to provide a short “user’s guide” to the wonderful
book Discriminants, Resultants and Multidimensional Determinants by Gelfand,
Kapranov and Zelevinsky [GKZ]. This book is full of interesting results and examples, very clearly explained. However, since the main results are developed in
great depth over several hundred pages, I felt there might be some value in writing
an outline of what I understand of their central argument, in case it helps others
navigate the book themselves.
I have tried to keep it as brief and as non-technical as possible while still providing
all the essential ideas for understanding the proof. This means that many topics are
skipped or covered very briefly, and of course my choice of what counts as ‘essential’
is highly subjective. Fortunately, the reader who wants to know more about any
topic already knows where they should look.
Acknowledgements. I would like to thank Hiroshi Iritani, for originally introducing me to this subject; Alex Kite and Will Donovan, for encouraging me to
actually read the book; and Michela Barbieri, for very helpful feedback on a draft.
1.1. The discriminant of a cubic. We all learned in kindergarten that the discriminant of a quadratic equation
aX 2 + bX + c
1

2

ED SEGAL

is ∆2 = b2 − 4ac. In the 3-dimensional space of quadratics there is a hypersurface
{∆2 = 0} where the quadratic has a repeated root.
The analogous question for cubic equations is much harder. The discriminant of
a cubic
aX 3 + bX 2 + cX + d
is the polynomial
∆3 = b2 c2 − 4ac3 − 4b3 d − 27a2 d2 + 18abcd

(1.1)

i.e. we have ∆3 (a, b, c, d) = 0 iff the cubic has a repeated root. This formula
was probably well-known to mathematicians of earlier centuries, but I suspect few
modern mathematicians could write it down without googling. The discriminant
of a quartic, ∆4 , is a polynomial with 16 terms.
The most important (in my view) result of [GKZ] is that in fact a lot of information about discriminants can be uncovered using a simple combinatorial game.
Let’s illustrate this procedure for the case of cubics. We start by drawing the Newton polytope of the cubic - the convex hull of the monomials involved. This is just
an interval:
aX 3
bX 2
cX
d
Next we subdivide this interval into smaller intervals. There are exactly four ways
we can do this:
a

b

c

d

a

c

d

a

b

d

a

d

The first subdivision is into three intervals each of length 1, the second and third
use an interval of length 2 and another of length 1, and the fourth subdivision uses
a single interval of length 3.
For each of these subdivisions we can produce a monomial in the variables
a, b, c, d. Here’s the rule: for each subinterval, consider the expression
length
length of subinterval × endpoints
(1.2)
and then multiply over each subinterval appearing. So the first of our subdivisions
gives the monomial
ab × bc × cd = ab2 c2 d
and the second subdivision gives:
(2ac)2 × cd = 4a2 c3 d
The third and fourth subdivisions give monomials 4ab3 d2 and 27a3 d3 respectively.
We have nearly recovered the discrimant of a cubic! If we divide by the common
factor of ad, and ignore the sign1, we have found four of the five terms in (1.1).
What of the fifth term in ∆3 ? To understand why this term is different from
the others we need to draw the Newton polytope of ∆3 itself. At first sight this
polytope lives in four dimensions so is hard to draw, but there are some obvious
symmetries that we can use to our advantage. Namely (i) scaling by an overall
constant will clearly not affect whether or not our cubic has a repeated root, and
(ii) neither will scaling the variable X by some factor. So the space of cubics C4a,b,c,d
carries an action of a torus (C∗ )2 under which ∆3 is invariant; the first factor acts
with weights (1, 1, 1, 1) and the second with weights (3, 2, 1, 0). If we allow ourselves
1The issue of signs is delicate, it is addressed in [GKZ] but we will not cover it in these notes.

A SHORT GUIDE TO GKZ

3

to pass to rational functions in a, b, c, d then it’s easy to compute that the invariants
are freely generated by
b2
c2
α=
and
β=
ac
bd
so in fact ∆3 must be (proportional to) a rational function in α, β. And indeed, a
little manipulation reveals that
∆3
= α2 β 2 − 4αβ 2 − 4α2 β − 27 + 18αβ
a2 d2
which has Newton polytope:
αβ 2

αβ

α2 β 2

α2 β

1
We can see that our combinatorial game has found the terms of ∆3 that live at
the vertices of the Newton polytope. We will refer to these as extremal terms. The
term 18αβ lives in the interior.
1.2. More general discriminants. It’s easy to check that the discriminant of a
quadratic appears when we play the same game starting with an interval of length
2. There are two possible subdivisions and they give us terms ab2 c and 4a2 c2 , then
dividing by ac (and inserting a minus sign) gives us ∆2 . Here all terms are extremal
since the Newton polytope of ∆2 is very small.
If we play the game for an interval of length 4 then we begin to discover the
discriminant of a quartic equation; it begins
∆4 = b2 c2 d2 ± 4ac3 d2 e ± 256a3 e3 ± ...
and one can easily write down another five terms coming from the remaining subdivisions. These eight terms will be the extremal terms of ∆4 .
More generally we can consider discrimants of polynomials in more than one
variable. The generalization of ‘having a repeated root’ is ‘defining a singular
hypersurface’.
Example 1.3. Consider the following polynomial in two variables:
f (X, Y ) = a + bX + cY + dXY
It defines a hypersurface (f ) ⊂ C2 , and it’s easy to calculate that (f ) has a singularity iff:
ad − bc = 0
So the discriminant of f is ad − bc. This is one of the rare examples where it is
easy to find the discriminant directly.
The Newton polytope of f is a square. The generalization of ‘subdividing into
intervals’ is ‘triangulating’, i.e. subdividing into simplices.2 Since there are two
possible triangulations
2We will only ever consider coherent triangulations [GKZ, p218]. In these notes ‘triangulation’
means ‘coherent triangulation’.

4

ED SEGAL

c

d

c

d

a

b

a

b

the combinatorics predicts that the disciminant of f will be:
(abd)(acd) ± (abc)(bcd) = abcd(ad ± bc)
Here we are using the formula (1.2) generalized to:
(volume of simplex × vertices)volume

(1.4)

Apart from the factor of abcd the game works, and there are no possible interior
terms.
If we have d variables X1 , ..., Xd and we fix a finite set
A ⊂ Nd
of monomials in these variables then there is a corresponding generic polynomial fA
with these terms, depending on n = |A| parameters a1 , ..., an . For the hypersurface
(fA ) ⊂ Cd to be singular we need d + 1 polynomials to vanish simultaneously, fA
and ∂i fA , ∀i. So we expect that (fA ) is smooth for a generic choice of parameters,
and singular for a codimension 1 locus in Cn . Assuming that this is true we define
the discriminant of A to be the polynomial
∆A ∈ C[a1 , ..., an ]
cutting out the locus where (fA ) is singular. As we see already for cubics in one
variable, computing this polynomial ∆A is going to be a difficult problem.
Remark 1.5. Finding discriminants is a special case of a more general problem called
‘resultants’. Fix d + 1 sets A0 , ..., Ad of monomials, all in the same set of variables
X1 , .., Xd . Then there is a corresponding list of polynomials g0 , ..., gd and for generic
values of the co-efficients we expect that the polynomials will have no common root.
But there will be (we expect) a hypersurface in parameter space where the gi ’s do
have a common root. The polynomial cutting out this hypersurface is called the
resultant.
Obviously if we can compute resultants then we can compute discriminants, by
setting g0 = f and gi = ∂Xi f . This idea will be important in Sections 2.3 and 2.4.
In some examples the naive dimension counting argument fails, and the subset
of the parameter space Cn where (fA ) is singular is not a hypersurface.
Example 1.6. Let d = 1 and A = {X 4 , X 3 , X 2 } so fA = aX 4 + bX 3 + cX 2 .
Obviously (fA ) is always singular at the origin, regardless of the values of a, b, c.
However, if we remove the origin then (fA ) is singular iff b2 − 4ac = 0.
This example shows that it is better to consider (fA ) as a hypersurface in the
torus (C∗ )d instead of Cd . If we do this then ∆A is not affected by translations of
the set A, and indeed we can allow fA to be a Laurent polynomial, i.e. with A ⊂ Zd
instead of Nd .
The next example is a little more subtle.
Example 1.7. Let d = 2 and A = {1, X, X 2 , Y }, so:
fA = a + bX + cX 2 + dY
Obviously (fA ) ⊂ (C∗ )2 is singular iff the parameters lie in the subset
{d = b2 − 4ac = 0}

A SHORT GUIDE TO GKZ

5

which has codimension two in C4 .
What does our combinatorial game predict for this example? The convex hull
of A has two possible triangulations
d

a

d

b

c

a

c

so the game predicts that the discriminant of A should be:
(abd)(bcd) ± (2acd)2 = acd2 (b2 ± 4ac)
It is apparent that this expression is not detecting the disciminant of fA itself,
but rather of the polynomial
fA′ = a + bX + cX 2
associated to the bottom face A′ ⊂ A.
In light of this we define a polynomial EA , which [GKZ] call the principal Adeterminant, by setting:
Y
EA =
(∆A′ )mA′
(1.8)
faces A′ ⊂A

Here:
• The product runs over all faces of A of all dimensions, including A itself.
• We formally declare ∆A′ ≡ 1 if the locus where (fA′ ) is singular is not a
hypersurface.
• The mA′ ’s are certain multiplicities that we will not address in these notes.
And then their main result is:
Theorem 1.9. [GKZ, p302]3 The combinatorial game described above computes
the extremal terms of EA .
We will sketch the proof of this theorem in Section 2. But before we start let’s
make a few elementary remarks:
• The discriminant locus {∆A = 0} is now just one of the irreducible components4 of a larger variety {EA = 0}. We call it the principal component,
and it may be empty as in Example 1.7. It’s often sensible to think of the
larger set {EA = 0} as the correct ‘discriminant locus’ associated to A.
• Only certain faces A′ ⊂ A will contribute non-empty components of the
discriminant. They can be identified by a simple combinatorial criterion,
see e.g. [KS, Sec. 4.2.2].
• If A′ is a vertex of A then fA′ is a single monomial, and ∆A′ is just the
coefficient of this monomial. So each variable lying at a vertex of A appears
as a factor of EA . For example, the principle A-determinant for a quadratic
in one variable will be
EA = ac(b2 − 4ac)

(1.10)

(all the multiplicities mA′ are 1 in this example). This explains the additional factors that appeared in all our examples above.
3We’re citing results in [GKZ] by page number because we found their numbering conventions

challenging. There are, for example, four separate results labelled Proposition 3.1.
4Each hypersurface {∆ ′ = 0} is irreducible by a simple geometric argument [GKZ, p15].
A

6

ED SEGAL

2. Proof of the main theorem
2.1. Projective duality. Discriminants are closely related to the geometric subject of projective duality, which is almost as classical and even more subtle.
Let V ⊂ Pn be a smooth projective variety. Write (Pn )∨ for the dual projective
space of hyperplanes in Pn . The projective dual of V is the subvariety:
V ∨ = {H ∈ (Pn )∨ | H is tangent to V }
Note that V ∨ is not intrinsic to V , it depends on the embedding V ,→ Pn . Also we
can extend the definition to singular V by declaring V ∨ to be the closure of the set
of hyperplanes which are tangent to a smooth point of V .
The key property of projective duality is that it is indeed a duality, i.e. the map
V 7→ V ∨ is an involution on the set of subvarieties of Pn . This is called the biduality
theorem and requires a little work to prove [GKZ, p15].
By a dimension count, as in Section 1.2, we see that we expect V ∨ to be a
hypersurface in (Pn )∨ . This is regardless of the dimension of V .
Remark 2.1. At first sight this fact seems odd given the biduality theorem; we have
an involution V ↔ V ∨ on subvarieties of Pn under which almost every subvariety
maps to a hypersurface. However, if V is not a hypersurface then V ∨ is a rather
special variety: it must be ruled by linear subspaces (because if codim V = c then
every smooth point of V has a Pc−1 of tangent hyperplanes). A generic deformation
of V ∨ will lose this property so must be projectively-dual to a hypersurface, not to
a deformation of V . So the moduli space of V ∨ typically contains the moduli space
of V as a subvariety of lower dimension. See Example 2.8.
Now we relate projective duality to disciminants. Fix, as in the previous section,
a lattice Zd and a finite subset A ⊂ Zd of size n. We will explain how to construct
a projective toric variety
VA ⊂ Pn−1
such that the discriminant ∆A is exactly the equation of the projective dual of VA .5
There a few different ways to express this construction. The simplest is to regard
our set of Laurent monomials A as defining an embedding
(C∗ )d ,→ Pn−1
and take VA to be the closure of the image.
Example 2.2. Returning to quadratics in one variable, set A = {X 2 , X, 1}. This
defines an embedding:
C∗ ,→ P2
x 7→ x2 : x : 1
The closure of the image is obviously the quadric curve (αγ−β 2 ), and the embedding
extends to
VA ∼
= P1 ,→ P2
2

x : y 7→ x : xy : y

5We shall see in Section 3.2 that V

(2.3)
2

A is one of various different toric varieties associated to the

data of A.

A SHORT GUIDE TO GKZ

7

More formally, we can embed Zd into Zd+1 as an affine hypersurface at height
1, and view A as a subset of Zd+1 . Then we can consider the cone
ΓA = ⟨A⟩N ⊂ Zd+1
i.e. the submonoid of Zd+1 generated by A. Then the monoid ring C[ΓA ] is a
graded ring generated in degree 1. We define VA to be the associated projective
variety:
VA = Proj C[ΓA ]
By construction this is a toric variety, equipped with an embedding into Pn−1 .
Example 2.4. If we set A = {X 3 , X 2 , X, 1} then it’s easy to verify that
C[ΓA ] = C[α, β, γ, δ] / (αγ − β 2 , βδ − γ 2 , αδ − βγ)
and hence VA ∼
= P1 again, but embedded into P3 as a twisted cubic curve:

(2.5)

x : y 7→ x3 : x2 y : xy 2 : y 3
The generalisation to higher degree polynomials f (X) is obvious.
Note that the inclusion
i : VA ,→ Pn−1
is a toric embedding, the image is not invariant under the full torus action on Pn−1
but it is preserved by a subtorus of rank d. Also the moment polytope of VA is just
the convex hull of A (see Section 2.4).
Now choose a hyperplane in H ⊂ Pn−1 . It is specified by a section of O(1), a
linear function with some coefficients a1 , ..., an . Pulling this section back along i
we get a section of a line bundle of VA , and in the big open torus (C∗ )d ⊂ VA this
is exactly our Laurent polynomial fA . Obviously
H is tangent to VA ⇐⇒ (fA ) has a singular point
and we conclude that ∆A is the equation for the projective dual of VA .
Remark 2.6. In this argument we’re ignoring points of VA that live outside (C∗ )d .
This makes no difference when computing the projective dual; the tangent hyperplanes to points in VA ∩ (C∗ )d provide a Zariski-open subset of (VA )∨ , and we know
that (VA )∨ is always irreducible.
Example 2.7. Set A = {X 2 , X, 1} so VA ∼
= P1 is a quadric curve in P2 (Example
2.2). A hypersurface H is defined by a linear form aα + bβ + cγ, and pulling this
back to VA gives a section of O(2), the homogeneous quadratic form:
aX 2 + bXY + cY 2
In the torus C∗ ⊂ VA this is fA (X). So the projective dual to VA is:
VA∨ = (b2 − 4ac) ⊂ P2
Hence the projective dual to any quadric curve is another quadric curve.
Example 2.8. The projective dual to a twisted cubic P1 ⊂ P3 (Example 2.4) is a
quartic surface in P3 , defined by the discriminant ∆3 of a cubic polynomial (1.1).
Note that this must be a special quartic surface since its projective dual is a
curve. If we take the projective dual of a general quartic surface (or even a general
quartic having the same Newton polytyope as ∆3 ) we will get a surface in P3 of
some high degree.

8

ED SEGAL

2.2. Toric degenerations. Finding projective duals or discriminants directly is
hard in most cases. An easier question is to look only for the extremal terms of
the discriminant, as discussed in Section 1. This means using the torus action on
projective space and studying projective duality ‘asymptotically’.
Let V ⊂ Pn be any subvariety of projective space. If we act by the torus (C∗ )n on
Pn we generate an n-parameter family of subvarieties, and the limits of this family
(once we’ve defined them properly) must be subvarieties which are torus-invariant.
We will call these limits the extreme toric degenerations of V .
The straightforward case is when V is a hypersurface.
Example 2.9. Let V ⊂ P2α:β:γ be the quadric curve cut out by:
F (α, β, γ) = α2 + αβ + β 2 + αγ + βγ
The torus action on P2 rescales the coefficients of F , moving them away from 1,
producing a 2-parameter family of quadric curves. It’s easy to see that the limits
of this family are the four reducible and torus-invariant quadrics
(α2 ),

(β 2 ),

(αγ),

(βγ)

corresponding to the four vertices of the Newton polytope of F . Indeed, a 1parameter subgroup of (C∗ )2 corresponds to a choice of dual vector n on the lattice
Z2 , and the limit of of V under that C∗ action corresponds to the vertex which
maximizes n. So these are the extreme toric degenerations of V .
Note that our V in this example is not generic because F has no γ 2 term, i.e. V
contains the torus fixed point 0 : 0 : 1. A generic quadric curve would have three
extreme toric degenerations (α2 ), (β 2 ), (γ 2 ).
It’s clear that all hypersurfaces behave in this way. We can compactify the
space of irreducible hypersurfaces using the space of effective divisors in Pn , i.e. by
(the projectivization of) the appropriate space of polynomials. And if we have a
hypersurface V = (F ) then we have a correspondence:
{Extreme toric degenerations of V } ↔ {Extremal terms of F }
When V is not a hypersurface the situation is more complicated. Obviously each
extreme toric degeneration must be a union of torus-invariant linear subspaces,
since no other subvarieties are torus-invariant. And if V is generic then it’s not too
hard to argue that every limit must actually be a single linear subspace, perhaps
with some multiplicity. But if V is non-generic then taking the limits requires more
thought. See Example 2.12.
Now suppose VA ⊂ Pn is one of our torically-embedded subvarieties from the
previous section. This is far from generic, in at least two ways:
(1) VA is preserved by a subtorus of rank d = dim VA so we only get an (n − d)parameter family of subvarieties.
(2) VA contains at least d + 1 of the torus fixed points in Pn , corresponding to
the vertices of A. Hence all subvarieties in the family, including the limits,
must contain all these fixed points.
Example 2.10. Let A = {X 2 , X, 1} as in Example 2.2. We can think of VA either
as the hypersurface (αγ − β 2 ) ⊂ P2 or as the image of P1 under the embedding
(2.3).
The extreme toric degenerations of VA are the reducible toric-invariant curves
cut out by αγ and β 2 . If we take the embedding viewpoint then the second limit

A SHORT GUIDE TO GKZ

9

corresponds to the double-cover
x : y 7→ y 2 : 0 : x2
of (β). But in the first limit our parametrized curve breaks into two separate
components (α) and (γ), and the limit is not parametrized by a single P1 .
Here’s a schematic drawing of this family with the original VA in blue. In one
limit we get two sides of the triangle, and in the other we get the remaining side
with multiplicity two.

Example 2.11. Let A = {1, X, Y, XY } as in Example 1.3. Then VA ∼
= P1 × P1 ,
3
embedded into P via the Segre embedding
(x : z, y : w) 7→ zw : xw : zy : xy
with image cut out by:
F (α, β, γ, δ) = αδ − βγ
Notice that VA contains all four toric fixed points and also two of the toric boundary
curves. Schematically we can think of it as a square (the convex hull of A) embedded
into a tetrahedron, as in this picture:

Acting via (C∗ )3 produces a 1-parameter family of subvarieties in P3 , and the
extreme toric degenerations are the reducible toric-invariant surfaces (αδ) and (βγ).
In our schematic picture the embedded square flows towards the boundary of the
tetrahedron, and in the limit breaks into a union of two boundary triangles. The
two possible limits correspond to the two possible triangulations of the square.
Example 2.12. Let A = {X 3 , X 2 , X, 1} as in Example 2.4, so VA ∼
= P1 embedded
3
∗ 3
as the twisted cubic in P . Acting by (C ) produces a 2-parameter family of
embedded curves
x : y 7→ x3 : sx2 y : txy 2 : y 3 ∈ P3
each containing the fixed points 1 : 0 : 0 : 0 and 0 : 0 : 0 : 1. Since this VA is not a
hypersurface defining the limits of this family is a bit harder, and we’ll wait until
the next section to treat this properly. But we can do some heuristic investigation
to find that there are four limits:
• In the limit (s, t) → (0, 0) we get a triple cover of the toric boundary curve
P1α:δ ⊂ P3 .

10

ED SEGAL

• If we send s → 0 we get a 1-parameter family of curves lying in the boundary
plane P2α:γ:δ , defined by (γ 3 −t3 αδ 2 ). At t → 0 we get P1α:δ with multiplicity
three, as above. At t → ∞ we get P1α:γ ∪ P1γ:δ where the first component
has multiplicity two.
• Similarly, setting t → 0 gives a 1-parameter family having a limit P1α:β ∪P1β:δ
where the second component has multiplicity two.
• With a little geometric imagination we can see that the final limit is:
P1α:β ∪ P1β:γ ∪ P1γ:δ
This appears when we send s → ∞ and t → ∞ simultaneously. An algebraic way to see it is to take the ideal of VA (2.5) and observe that it can
degenerate to (αγ, βδ, αδ).6
Schematically the original VA is an interval embedded into a tetrahedron as a curved
path joining two of the vertices. When we flow it to the boundary it breaks into
unions of edges. There are four ways this can occur and they correspond to the
four subdivisions of the interval we saw in Section 1.1.
We can perform this procedure for any of our torically-embedded varieties VA .
In the limits VA will break into unions of parts of the toric boundary of Pn , with
some multiplicities. Given the previous examples, the following claim should be
very plausible.
Claim 2.13. The extreme toric degenerations of VA correspond to triangulations
of the convex hull of A, and the multiplicity of each component matches the volume
of the corresponding simplex.
This is perhaps the key geometric ingredient in Theorem 1.9. Indeed we now
have enough to attempt a sketch proof of the theorem, as follows:
{Triangulations of A} ↔ {Extreme toric degenerations of VA }
↔ {Extreme toric degenerations of VA∨ }
↔ {Extremal terms of ∆A }
Of course there are still glaring issues to address, including the fact that we haven’t
defined extreme toric degenerations rigorously yet (beyond the hypersurface case),
and the difference between ∆A and EA . But there is also important flaw in this
sketch in the second step, because taking projective duals does not behave well with
respect to toric degenerations.
Example 2.14. We saw in Example 2.10 that the projective dual of the quadric
curve V = (β 2 − αγ) ⊂ P2 is another quadric curve V ∨ = (b2 − 4ac). Also, one of
the extreme toric degenerations of V is the reducible linear curve V∞ = (αγ).
The projective dual of V∞ is obviously two points, namely the two hypersurfaces
(α) and (γ) viewed as points:
{1 : 0 : 0, 0 : 0 : 1} ⊂ (P2 )∨
But the extreme toric degenerations of V ∨ are curves.
This example illustrates a general phenomenon: an extreme toric degeneration
is always a union of linear subspaces, so its projective dual will never be a hypersurface. We will repair this in the next section by replacing projective duals with
another related notion.
6Although we are actually discussing limits in the Chow variety, as we’ll see in the next section,

and taking limits of ideals would mean working in the Hilbert scheme. In the Hilbert scheme there
are more possible limits but some of them get identified under the Hilbert-Chow morphism.

A SHORT GUIDE TO GKZ

11

2.3. Associated hypersurfaces. Let V ⊂ Pn−1 be a subvariety of projective
space of dimension d. We’ve seen how to define the projective dual V ∨ by taking
linear hypersurfaces and seeing which ones fail to intersect V generically. There are
many variations on this idea.
For instance, instead of the dual projective space we can consider the Grassmannian Gr(n − d − 1, n) of linear subspaces Pn−d−2 ,→ Pn−1 of codimension d + 1. A
generic such subspace will not intersect V at all, but we can consider the subset of
those which do intersect V , and by another dimension count we see that we expect
this subset to be a hypersurface in the Grassmannian. We call it the associated
hypersurface:

ZV = S ∈ Gr(n − d − 1, n) | S ∩ V ̸= ∅
This construction is also a duality, the variety V can be recovered from its associated
hypersurface ZV [GKZ, p102]. And it is better behaved than projective duality in
that the naive dimension count essentially never fails; if V is irreducible then ZV
will always be an irreducible hypersurface in the Grassmannian [GKZ, p99].
We extend the definition to reducible varieties by treating them as formal sums
of irreducible varieties, i.e. as effective algebraic cycles, and then the associated
hypersurface is the corresponding effective divisor in the Grassmannian.
We are interested in the equation cutting out ZV . However, we need to explain
exactly what we mean by this, and in fact there are two slightly different options.
A point in S ∈ Gr(n − d − 1, n) is a subspace defined by d + 1 linear equations,
i.e. by a matrix
M ∈ Matn×(d+1) (C)
having full rank. The Grassmannian itself is the quotient of this space of matrices
by the group GLd+1 (C). This is of course the canonical isomorphism
Grsub (n − d − 1, n) ∼
= Grquot (n, d + 1)
between a Grassmannian of subspaces of Cn and a Grassmannian of quotient spaces
of Cn . So our associated hypersurface ZV is the zero locus of some polynomial in
n × (d + 1) variables (the entries of M ) which is invariant under GLd+1 (C). We
call this polynomial the V -resultant and denote it by RV .7
A basic fact about Grassmannians is that any such RV must be expressible as
a polynomial in the maximal minors of M . These are the Plücker co-ordinates,
i.e. we use the Plücker embedding
Gr(n, d + 1) ,→ P(∧d+1 Cn )
and then ZV must in fact be the intersection of the Grassmannian with a hypersurface in the ambient projective
 space. So a second point-of-view is that ZV is
n
defined by a polynomial in d+1
variables. This is called the Chow form and we
denote it RV .
Example 2.15. Suppose V is a linear subspace Pk ⊂ Pn . It’s easy to see that ZV is
then a linear hypersurface, i.e. RV is a linear function in the Plücker co-ordinates.
So RV is a polynomial of degree d + 1.
Note that individial Plücker co-ordinates correspond to the linear subspaces Pk ⊂
Pn which are torus invariant.
Example 2.16. Suppose V is a hypersurface in Pn−1 . Then obviously ZV is just V
itself. So RV is just the equation defining V , under the identification of Pn−1 with
Gr(n, n − 1). But RV is an equation of much higher degree, obtained by replacing
each co-ordinate in Pn−1 with the corresponding minor of M .
7[GKZ] use the notation R̃ . The reason for the name will become clear shortly.
V

12

ED SEGAL

Now we specialize this construction to the case when V is one of our toricallyembedded subvarieties:
i : VA ,→ Pn−1
Each column of our matrix M is a section of O(1) on Pn−1 , i.e. a linear hypersurface,
and the intersection of these hypersurfaces is our subspace S. Pulling these sections
back along i we get (d + 1) sections of a line bundle on VA . In the big open torus
(C∗ )d ⊂ VA these are Laurent polynomials g1 , ..., gd+1 , all involving the same set of
monomials A. It is evident that
VA ∩ S ̸= ∅ ⇐⇒ g1 , ..., gd+1 have a common root.
So the polynomial RA := RVA is exactly the resultant of this set of polynomials,
see Remark 1.5.
Remark 2.17.
(1) Since the polynomials g1 , ..., gd+1 all involve the same set of monomials this
is a homogenous resultant. The general case, involving different sets of
monomials, is called a mixed resultant.
(2) Previously we weren’t exactly clear about where the common root should
lie but now we can be precise: the resultant RA vanishes iff g1 , ..., gd+1 have
a common root in the toric variety VA . This requires making the Laurent
polynomials into homogenous polynomials in the right way, see Section 2.4.
Example 2.18. Let A = {X 2 , X, 1} so VA ∼
= P1 is the quadric curve (αγ−β 2 ) ⊂ P2 .
We consider
S ∈ Gr(1, 3) ∼
= Gr(3, 2)
defined by a matrix:


a1 a2
M =  b1 b2 
c1 c2
The columns of M are two linear forms on P2 , which on VA become two quadratic
forms:
g1 = a1 X 2 + b1 XY + c1 Y 2 ,
g2 = a2 X 2 + b2 XY + c2 Y 2
So S intersects VA iff these two quadratics have a common root in P1 .
But this is of course a special case of Example 2.16: since VA is a hypersurface
S is just a point, and the associated hypersurface ZA := ZVA is just VA itself.
Explicitly we have that
S = (b1 c2 − c1 b2 ) : (c1 a2 − a1 c2 ) : (a1 b2 − b1 a2 )
and substituting this into the equation for VA gives us the resultant:


2
RA = b1 c2 − c1 b2 a1 b2 − b1 a2 − c1 a2 − a1 c2

(2.19)

The two quadratics have a common root iff this expression vanishes.
Example 2.20. Let A = {X 3 , X 2 , X, 1} so VA is the twisted cubic curve P1 ,→ P3
(Example 2.4). The associated hypersurface ZA lies in Gr(2, 4) ⊂ Mat4×2 /GL2 , it
is the subset where two generic cubics
g1 = a1 X 3 + b1 X 2 Y + c1 XY 2 + d1 Y 3 ,

g2 = a2 X 3 + b2 X 2 Y + c2 XY 2 + d2 Y 3

have a common root. So the polynomial RA is the resultant of the two cubics.
However, since VA is not a hypersurface, computing this resultant is not such
an easy problem. Indeed if we could solve this problem then we could immediately
deduce the discriminant of a single cubic.

A SHORT GUIDE TO GKZ

13

Chow forms give us a way to take limits in the space of subvarieties in Pn−1 , and
in particular to give a rigorous definition to the extreme toric degenerations considered in the previous section. The action of (C∗ )n−1 on Pn−1 induces a torus action
on any Gr(k, n), and hence on the space of effective divisors in the Grassmannian.
Given a subvariety V , suppose that the associated hypersurface
ZV ⊂ Gr(n − d − 1, n)
has a toric-invariant limit (ZV )∞ in the space of effective divisors. We define
the corresponding extreme toric degeneration of V to be the subvariety V∞ whose
associated hypersurface is (ZV )∞ .
We can also express this in terms of the defining equation for ZV . Our torus
action lifts to an action of (C∗ )n on Matn×(d+1) (C) just by scaling each row of the
matrix. Each monomial in RV has a weight under this action, and we call the set
of such weights (or perhaps its convex hull) the Chow polytope of V . Note that
every monomial contributing to a given maximal minor will have the same weight,
so it doesn’t matter if we use RV or the Chow form RV for this definition.
Even after passing to RV it can still be the case that several monomials have
the same weight, since we are not using the full torus action on P(∧n−2−1 Cn ). So
the Chow polytope is not the Newton polytope of RV , but only a projection of it.
Nevertheless we claim that the vertices of the Chow polytope must correspond to
individual monomials in RV . So we can call these the ‘extremal terms’ of RV .
To understand this claim, suppose that V degenerates to some torus-invariant
V∞ . Then V∞ must be a union of torus-invariant linear subspaces, perhaps with
some multiplicities. For each such subspace the associated hypersurface is cut out
by a single Plücker co-ordinate (Example 2.15). So by definition, the associated
hypersurface to V∞ is the divisor given by the corresponding monomial in Plücker
co-ordinates. This monomial is one of the extremal terms of RV .
Example 2.21. Let A = {1, X, X 2 , Y, XY }. Then VA is a toric surface embedded
into P4 , the associated hypersurface ZA lives in Gr(2, 5), and computing its defining
equation RA is the same problem as finding the resultant of three polynomials
g1 , g2 , g3 where:
gi = ai + bi X + ci X 2 + di Y + ei XY
As we argued in the previous section, extreme toric degenerations of VA correspond
to triangulations of A. For example the triangulation
d

e

a

b

c

corresponds to the degeneration:
(VA )∞ = P2α:β:δ ∪ P2β:γ:δ ∪ P2γ:δ:ϵ

⊂ P4α:β:γ:δ:ϵ

The linear subspace P2α:β:δ has associated hypersurface cut out by the single Plücker
co-ordinate


a1 a2 a3
πabd := det  b1 b2 b3 
d1 d2 d3
and similarly for the other two components. So the associated hypersurface for
(VA )∞ is the divisor in Gr(2, 5) defined by:
πabd πbcd πcde

14

ED SEGAL

Hence this monomial must be one of the extremal terms of RA . There are four
other triangulations of A and they give four more extremal terms:
πabd πbde πbce ,

πade πabe πbce ,

πade (πace )2 ,

(πacd )2 πcde

We can now formulate Claim 2.13 more precisely as:
Theorem 2.22. [GKZ, p260] The extremal terms of RA biject with triangulations
of A. For a given triangulation, the corresponding monomial is
Y
′
± (πA′ )vol(A )
where the product runs over the simplices A′ of the triangulation and πA′ denotes
the corresponding Plücker co-ordinate.
Although this result has a clear geometric motiviation the actual proof is entirely
algebraic. In Section 3.1 we explain the beginnings of the method they use.
Note that the fact that the extremal terms are monic (up to sign) is important,
but we won’t attempt to justify it here.
2.4. From Chow forms to discriminants. In this section we fill in the final
steps connecting Theorem 2.22 to Theorem 1.9, and in the process discover how
the principal A-determinant EA arises in the story.
Example 2.23. In Example 2.18 we found the formula (2.19) for the resultant of
two general quadratic polynomials g1 , g2 , which in our new notation we could write
as:
RA = πab πbc − (πac )2
To get the discriminant of a single quadratic f = aX 2 + bX + c we compute the
resultant of g1 = f and g2 = ∂Xf = 2aX + b, i.e. we specialize variables




a1 a2
a 0
 b1 b2  ⇝  b 2a
c1 c2
c b
and hence:
πab 7→ 2a2 ,

πbc 7→ b2 − 2ac,

πac 7→ ab

Substituting this into RA we do indeed recover the discriminant ∆2 of a quadratic
(times a factor of a2 ).
However, we can see that this computation would be tidier if we were to use the
logarithmic derivative
g2 = X∂Xf = 2aX 2 + bX
instead of the usual ∂Xf . Then we’d apply a different specialization




a1 a2
a 2a
πab 7→ −ab
 b1 b2  ⇝  b b 
πbc 7→ −bc
c1 c2
c 0
πac 7→ 2ac
and the connection to ∆2 is more obvious. Notice that this with approach the result
is actually ac∆2 , which is precisely EA (1.10).
In more complicated examples this use of the logarithmic derivative is not only
tidier, it’s actually essential if we want to get any information.
Example 2.24. Let A = {1, X, X 2 , Y, XY } as in Example 2.21. There we considered three polynomials g1 , g2 , g3 with this Newton polygon and studied their
resultant RA , finding the five extremal terms.

A SHORT GUIDE TO GKZ

15

Now suppose we want to find the discriminant of a single polynomial:
fA = a + bX + cX 2 + dY + eXY
The obvious thing to do is to set g1 = fA and g2 = ∂XfA and g3 = ∂Y fA , so we
specialize RA under:




a b d
a1 a2 a3
 b 2c e
 b1 b2 b3 




 c1 c2 c3  ⇝  c 0 0




d e 0 
d1 d2 d3 
e 0 0
e1 e2 e3
But then the minors πace , πbce and πcde will all specialize to zero, and unfortunately all the extremal terms of RA contain one of these three Plücker co-ordinates
(since in any triangulation the edge ce must appear in one of the triangles). So this
process tells us nothing at all about ∆A .
Geometrically, this specialization is a map
P4 → Gr(2, 5)
which is not equivariant for the torus actions, so it’s not surprising that it doesn’t
work well with our method.
Before we continue with the previous example let’s recall a little more toric
geometry, so that we can understand logarithmic derivatives more clearly.
We have defined the toric variety VA as the closure of a torus embedded in Pn−1 ,
or as Proj of the monoid ring C[ΓA ]. However, it can also be constructed as a (GIT)
quotient of a vector space by a torus, and this description has some advantages.
For each codimension 1 face of the polytope A we consider the primitive normal
vector, this gives us a set of vectors {v1 , ..., vN } ⊂ Zd . These are the rays of the
toric fan for VA , which is the normal fan to A. Together these vectors define a map
ZN → Zd , which has a kernel K : ZN −d ,→ ZN , expressing the relations between
the vi ’s.
The variety VA is a quotient of the vector space CN by the action of the torus
∗ N −d
(C )
having weight matrix K.8 The co-ordinates X1 , ..., XN on CN are natural
‘projective co-ordinates’ on VA .
A section of a line bundle on VA is given by a polynomial in the Xi ’s which
is semi-invariant under (C∗ )N −d , i.e. it is quasi-homogeneous in N − d different
ways. We have seen that our Laurent polynomial fA becomes a section of such
a line bundle, which appears when we pull-back a generic section of O(1) via the
embedding i : VA ,→ Pn−1 . So there must be natural way to complete fA to a
multi-quasi-homogenous polynomial in N variables. And there is; you assign each
element of A a multi-degree by measuring its lattice distance from each codimension
1 face, and multiply the corresponding monomial accordingly.
Example 2.25. If d = 1 then we only have one variable and A is an interval of
some length l. Hence it has two normal vectors v1 = 1 and v2 = −1 with one
relation v1 + v2 = 0. So N = 2 and VA is the quotient of C2 by C∗ acting with
weight (1, 1). Hence VA ∼
= P1 . We saw this already in Example 2.4.
The Laurent polynomial fA (X) becomes a homogeneous polynomial fA (X, Y )
of degree l, which is a section of the line bundle O(l) on P1 .

8This is a GIT quotient, so we need to delete a certain closed subset of CN before quotienting.

16

ED SEGAL

Example 2.26. Take A = {1, X, X 2 , Y, XY } as in Examples 2.21 and 2.24. Then
A has four normal vectors v1 , ..., v4 ∈ Z2 which satisfy two relations:
v2
v1
v4

v2 + v3 = 0
v1 + v2 + v4 = 0


K=

0
1

1
1

1
0

0
1



v3

Hence the toric surface VA is a quotient of C4 by the action of (C∗ )2 with weights
K. The Laurent polynomial fA (X, Y ) becomes the homogeneous polynomial
fA (X, Y, Z, W ) = aZW 2 + bXZW + cX 2 Z + dY W + eXY
where the degrees in X, Y, Z, W correspond to the distances along v1 , v2 , v3 , v4 . This
polynomial is degree one in (Y, Z) and degree two in (X, Y, W ), and it defines a
section of a particular line bundle on VA .
Once we have homogenized fA as above we have N different logarithmic derivatives Xi ∂Xi fA we can consider, and each one has the same quasi-homogeneity
properties as fA . So they are sections of the same line bundle that fA is a section
of.9
Now suppose we look for a common root of fA and all its logarithmic derivatives.
These are N + 1 polynomials but they are not linearly independent; each quasihomogeneity of fA is exactly a relation between them. So we are only really asking
for a common root of d + 1 polynomials, any choice of basis for the lattice:
fA , X1 ∂X1fA , ..., XN ∂XNfA Z

(2.27)

As usual there should be a hypersurface
∇ ⊂ (Pn−1 )∨
in the space of coefficients such these d + 1 polynomials have a common root in VA .
The equation for ∇ is the resultant of the polynomials.
Moreover there are different ways to solve the set of equations (2.27) so we expect
∇ to have several irreducible components.
One option is to find a critical point of fA : this is the projective dual of VA , the
vanishing of the discriminant ∆A . We call this the principal component of ∇ (as
mentioned in Section 1).
But we can also solve the equations by first setting some subset of the Xi ’s to zero,
and then looking for a critical point of the polynomial fA in the remaining variables.
Setting an Xi to zero corresponds to looking at a codimension 1 face A′ ⊂ A of the
polytope and considering the associated polynomial fA′ , or more geometrically, it
means restricting fA to the toric boundary divisor VA′ = {Xi = 0} ⊂ VA . Then
asking for a critical point means looking at the projective dual of VA′ , i.e. the
vanishing of the discriminant ∆A′ . If we set several Xi to zero we are considering
a face A′ ⊂ A of higher codimension.10
Putting all the components together, we see that ∇ is exactly the vanishing locus
of the principal A-determinant EA (1.8), so EA is the resultant of the polynomials
(2.27).
9Geometrically there is no differentiation operation on sections of a line bundle without some

additional structure. Here we are using the structure of VA being a toric variety (or at least the
choice of toric boundary).
10We might choose a set of X such that the intersection of the corresponding codimension 1 faces
i
of A is empty. But in this case the intersection of the corresponding toric boundary divisors in
VA is also empty, so our polynomials won’t have a common root.

A SHORT GUIDE TO GKZ

17

Example 2.28. Now we can finish Example 2.26. We are interested in finding
common roots in VA of the five polynomials:

fA , X∂XfA , Y ∂Y fA , Z∂ZfA , W ∂WfA
(2.29)
And we have that
Y ∂Y fA + Z∂ZfA = fA

and

X∂XfA + Y ∂Y fA + W ∂WfA = 2fA

so the vanishing of any three implies the vanishing of all five. There are 6 ways
solve these equations, so ∇ has 6 irreducible components. We can either:
• Find a critical point of fA . This gives the principal component {∆A = 0}.
• Set Y = 0 and find a critical point of fA′ = aW 2 + bXW + cX 2 .
• Set X = Y = 0 and a = 0. Or the corresponding solution for the other
three vertices of A.
So ∇ is cut out by the reducible polynomial:
EA = acde(b2 − 4ac)∆A (a, b, c, d, e)
On the other hand, we know that EA can be computed as the resultant of three
polynomials, so we can compute its extremal terms using Theorem 2.22. Indeed we
already computed the extremal terms of RA in Example 2.21.
If choose our three polynomials to be
g1 = fA ,

g2 = X∂XfA ,

g3 = Y ∂Y fA

(2.30)

then we are specializing the variables of RA as follows:




a1 a2 a3
a 0 0
 b1 b2 b3 
 b b 0




 c1 c2 c3  ⇝  c 2c 0




d1 d2 d3 
d 0 d
e e e
e1 e2 e3
Under this specialization each Plücker co-ordinate maps to the product of the corresponding variables, multiplied by some integer. For example:


1 0 0
πacd 7→ acd × det1 2 0 = 2acd
1 0 1
Two facts are clear:
(1) These expressions are independent (up to sign) of our particular choice of
polynomials (2.30), they only depend on the lattice spanned by (2.29).
(2) The coefficient of each Plücker co-ordinate is the volume of the corresponding simplex in A.
Combining (2) with Theorem 2.22, we see that the extremal terms of EA are computed by precisely the combinatorial game (1.4) introduced in Section 1.
The last two facts, and hence the conclusion, are evidently true in all examples.
This completes our sketch proof of Theorem 1.9.
Remark 2.31. In the previous example it’s actually not hard to compute ∆A by
hand. It’s given by
∆A = ae2 − bde + cd2
i.e. the hypersurface (fA ) is singular iff the root of the linear equation d + eX also
satisfies the quadratic a + bX + cX 2 . So the extremal terms of EA can be computed
directly and Theorem 1.9 verified in this example. We invite the reader to try this
exercise.

18

ED SEGAL

3. Other topics
In this final section we cover a couple of additional topics that may be of interest.
In the previous sections we tried to assume as little background as possible; from
now on we make no such attempt.
3.1. The Cayley method. In 1848 Cayley discovered a beautiful method for computing resultants exactly. It is not computationally efficient but it provides an
essential theoretical tool for the proofs in [GKZ].
Let E be a vector space of dimension n and s ∈ E an element. The Koszul
complex associated to s is the chain complex:
s
s
s
s
∧n E ∨ −→
... −→ ∧2 E ∨ −→ E ∨ −→ C

This complex is exact, unless s = 0. More generally if E is a vector bundle over
some base variety B, and s is a section of E, then the Koszul complex (∧• E ∨ , s)
is a chain complex of vector bundles over B. It is exact away from the vanishing
locus of s. In fact if s is a transverse section then the Koszul complex has homology
only at the final term, so it is a free resolution of the torsion sheaf Os=0 on B.
Example 3.1. Let
B = C6a1 ,b1 ,c1 ,a2 ,b2 ,c2 × P1x:y
and set E = O(2)⊕2 . We have a tautological section of E given by two generic
quadratics:
s = (g1 , g2 ) = (a1 x2 + b1 xy + c1 y 2 , a2 x2 + b2 xy + c2 y 2 ) ∈ Γ(B, E)
The vanishing locus {s = 0} is the set of points where x : y is a root of both g1 and
g2 . The Koszul complex (∧• E ∨ , f ) is a free resolution of the sheaf Os=0 .
Now consider the projection π : B → C6 . The push-down π∗ Os=0 is a torsion
sheaf, supported on the locus in parameter space where f1 and f2 have a common
root. So if we write R2 for the resultant of the two quadratics then this sheaf is
supported on the subvariety:
{R2 = 0} ⊂ C6
To get a free resolution of this sheaf we can take the derived push-down of the
Koszul complex on B, i.e. take the cohomology of each bundle ∧p E ∨ over the P1
fibres. We can make life easier by first twisting by the line bundle O(3), so the
Koszul complex becomes
s

s

O(−1) −→ O(1)⊕2 −→ O(3)
which is a free resolution of the sheaf Os=0 (3). Pushing down this sheaf gives,
again, some torsion sheaf supported on {R2 = 0}, and it has a free resolution
D

0 −→ O⊕4 −→ O⊕4 −→ π∗ (Of =0 (3))

(3.2)

because:
• Rπ∗ O(3) = H 0 (P1 , O(3)) ⊗ OC6 ∼
= (OC6 )⊕4
• Rπ∗ O(1) = H 0 (P1 , O(1)) ⊗ OC6 ∼
= (OC6 )⊕2
• Rπ∗ O(−1) = 0, since H p (P1 , O(−1)) = 0 for all p.
The differential D in (3.2) is some 4×4 matrix of polynomials in a1 , ..., c2 . And since
the cokernel of D is supported on the locus where D drops in rank, we conclude:
det D = R2

A SHORT GUIDE TO GKZ

19

This provides another way to compute the resultant R2 . Indeed, using the obvious
bases X, Y for H 0 (P1 , O(1)) and X 3 , X 2 Y, XY 2 , Y 3 for H 0 (P3 , O(3)) we can easily
compute that


a1 b1 c1 0
 0 a1 b1 c1 

D=
a2 b2 c2 0 
0 a2 b2 c2
and it’s easy to verify that det D agrees with the expression (2.19).
We can use the same technique to compute resultants of higher degree.
Example 3.3. Let Rd denote the resultant of a pair of degree d polynomials:
g1 (X, Y ),

g2 (X, Y )

So Rd is a polynomial in 2(d + 1) variables. We set
B = C2(d+1) × P1
and then s = (g1 , g2 ) is a section of a vector bundle E = O(d)⊕2 . We take the
Koszul complex of s, twisted by O(2d − 1), to get an exact sequence
s

s

0 −→ O(−1) −→ O(d − 1)⊕2 −→ O(2d − 1) −→ Os=0 (2d − 1) −→ 0
and then pushing-down to the coefficient space C2(d+1) gives a free resolution:
D

0 −→ O⊕2d −→ O⊕2d −→ π∗ (Os=0 (2d − 1)) −→ 0
Here π∗ (Os=0 (2d − 1)) is some torsion sheaf supported on {Rd = 0}. We conclude
that det D must equal Rd (or at least some power of Rd ).
For example, setting d = 3 we get the resultant of two cubics as:


a1 b1 c1 d1 0 0
 0 a1 b1 c1 d1 0 


 0 0 a1 b1 c1 d1 

R3 = det 
a2 b2 c2 d2 0 0 


 0 a2 b2 c2 d2 0 
0 0 a2 b2 c2 d2
By specializing the variables appropriately (as in Section 2.4) you can compute
the discrimant ∆3 by hand. For larger d it’s an easy problem for a computer algebra
system.
We can compute more general homogenous resultants RA by the same technique.
Take a set of polynomials g0 , ..., gd each using the same set of monomials A ⊂ Zd .
Then we have a toric variety VA , and after homogenenising correctly each gi is a
section of some line bundle L on VA . We take the space
B = C(d+1)|A| × VA
and consider the tautological section:
s = (g0 , ..., gd ) ∈ Γ(B, L⊕(d+1) )
The push-down of the Koszul resolution of Os=0 gives a complex of vector bundles
on coefficient space, with homology supported on {RA = 0}. This complex will
usually have length > 2, so we cannot compute RA as the determinant of a single
matrix, but it can be computed as the ‘determinant of a complex’ [GKZ, Appendix
A].

20

ED SEGAL

3.2. Mirror symmetry. A modern context in which an algebraic geometer might
encounter discriminants is mirror symmetry.11 This is a huge topic and we’ll say
very little about it, just enough to explain how discriminants are relevant.
Mirror symmetry is, at heart, an abstract duality acting on a certain class of
quantum field theories. In mathematics it is usually a duality on Landau-Ginzburg
models, this means a pair (Y, W ) consisting of a Kähler manifold Y and a holomorphic function W : Y → C. What makes it particularly interesting is that it
swaps the roles of the complex and symplectic structures; properties of the complex/algebraic geometry of (Y, W ) are reflected by the symplectic geometry of its
mirror (Y̆ , W̆ ).
The simplest class of examples involve toric geometry. Suppose we choose Y to
be simply a torus
Y = (C∗ )d
and W some Laurent polynomial. Then W is determined by some set of monomials
A ⊂ Zd , plus the choice of co-efficients. The mirror to (Y, W ) will be a toric
variety Y̆ determined combinatorially by A, equipped with the zero superpotential
W̆ ≡ 0.12
To build this toric variety Y̆ we view A as a map Zn → Zd , where n = |A|, and
find its kernel:
Q : Zn−d → Zn
Then we take a GIT quotient of Cn by the action of the torus (C∗ )n−d having
weight matrix Q. This gives a d-dimensional toric variety whose fan ‘refines A’, in
the sense that the one-dimensional cones of the fan are the rays spanned by (some
subset of) the elements of A.
Remark 3.4. This construction of Y̆ is similar to the construction of VA described in
Section 2.4, but it is important to understand that they are different toric varieties.
The fan for VA is the normal fan to the convex hull of A, so its rays come from the
normal vectors to this polytope, not the elements of A. Also VA is always compact
but Y̆ may not be.
In fact from this data Q there usually be more than one possible GIT quotient we
can form because GIT requires a choice of some extra data, a stability condition.13
So there will be some number of different toric varieties
Y̆1 , ..., Y̆t
whose fans are different possible refinements of A. These varieties are all birational.
This non-uniqueness is an important part of the mirror symmetry story. Varying
the coefficients of W should correspond to varying the Kähler class on the mirror
variety, since mirror symmetry exchanges complex moduli for symplectic moduli.
And in the space of quantum field theories, large variations of the Kähler class can
mean passing to another birational model. This leads to the concept of the stringy
Kähler moduli space which unifies the Kähler moduli spaces of all the Y̆i ’s.
Much of mirror symmetry involves understanding how certain quantities (GromovWitten invariants, derived categories, ...) vary over this moduli space. Unfortunately the space itself has no rigorous definition in general, so if we want to compute
examples we usually stick to toric varieties, where the stringy Kähler moduli space
11E.g. the author.
12This is a recipe due originally to Witten and Hori-Vafa. Note that we are not specifying the

Kähler metric on Y̆ , which is a much harder question.
13When we build V as a GIT quotient there is a canonical stability condition that we use
A
implicity.

A SHORT GUIDE TO GKZ

21

can be defined to be the coefficient space of the mirror W . So to do these kinds of
computations it is essential to understand the discriminant locus of W .
As a final remark, this story works particularly cleanly in the case that each Y̆i
is a Calabi-Yau variety. This happens when the torus (C∗ )n−d acts inside the the
special linear group SLn (C), which is a condition on the matrix Q, and is equivalent
to the monomials A ⊂ Zd all living in some affine hypersurface of height 1. This
says that W can be written in the form:
W = X0 fA (X1 , ...., Xd )
In this situation the fans for the Y̆i ’s correspond exactly to triangulations of the
convex hull of A (to turn a triangulation into a fan we just take the cone on each
simplex). So Theorem 1.9 gives us some relationship between the asymptotics of
EA and the different birational models Y̆i of our toric Calabi-Yau.
The precise statement involves yet another toric variety associated to A. Back in
Section 1.1 we observed that the discriminant of a cubic ∆3 can be sensibly understood as a rational function in two variables, by scaling out some symmetries. By
the same process a general ∆A can be reduced to a function of n − d variables, by
quotienting by a torus of rank d. More precisely, we should understand the discriminant locus {EA = 0} as a subvariety in some projective toric variety of dimension
n − d. This is the secondary toric variety; its toric data encodes the triangulations
of A, and in particular it has one toric fixed point for each Y̆i . Theorem 1.9 says
that the moment polytope of this secondary toric variety is exactly the Newton
polytope of EA , i.e. the asymptotics of {EA = 0} are dual to the toric boundary.
References
[KS]

A. Kite, E. Segal, Discriminants and semi-orthogonal decompositions, Comm. Math. Phys.
390 (2022), 907-931. arXiv:2102.08412
[GKZ] I. Gelfand, M. Kapranov, A. Zelevinsky, Discriminants, Resultants, and Multidimensional
Determinants, Birkhäuser Boston, 1994

